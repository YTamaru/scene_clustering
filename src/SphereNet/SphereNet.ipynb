{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skorch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_classlist = ['4gokan', '5gokan', '5gokan-denkisogokan', '5gokan-mediacenter'\n",
    "                    '8gokan', 'denkisogokan', 'lab', 'lab-lounge', 'lounge']\n",
    "s_classlist = ['4gokan_inside', '4gokan_outside', '5gokan-denkisogokan', '5gokan-mediacenter',\n",
    "'5gokan_1F', '5gokan_2F', '5gokan_3F', '5gokan_ent_east', '5gokan_lounge',\n",
    "'5gokan_out_east', '5gokan_parking', '5gokan_smoking', '5gokan_stairs_cnt',\n",
    "'5gokan_stairs_west', '8gokan_1F', '8gokan_ent_north', '8gokan_ent_south',\n",
    "'denkisogokan_2F', 'denkisogokan_3F', 'denkisogokan_4F', 'denkisogokan_elevator',\n",
    "'denkisogokan_lounge', 'denkisogokan_stairs', 'lab-lounge', 'lab_bs_cnt',\n",
    "'lab_corner', 'lab_desk', 'lab_desk_table', 'lab_ent', 'lab_printer',\n",
    "'lab_table', 'lab_wb_cnt', 'lab_wb_ent', 'lounge']\n",
    "num_l_class =len(l_classlist)\n",
    "num_s_class = len(s_classlist)\n",
    "print(num_s_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/'\n",
    "csv_path = os.path.join(DATA_FOLDER, 'frames_data.csv')\n",
    "datalist = pd.read_csv(os.path.join(DATA_FOLDER, 'frames_data.csv'), names=[\"img_path\", \"l_class\", 's_class','timestamp'])\n",
    "datalist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = datalist.drop(['s_class','timestamp'], axis=1)\n",
    "dfl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.groupby('l_class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = datalist.drop(['l_class', 'timestamp'], axis=1)\n",
    "dfs.groupby('s_class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.img_path.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = dfs[dfs['s_class'].str.startswith('lab')]\n",
    "num_labs_class = 10\n",
    "labs.s_class.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_test_img_path =[\n",
    "    \"../data/insta_frames/4gokan/4gokan_inside/4gokan_inside_0_img_000240.png\",\n",
    "    \"../data/insta_frames/4gokan/4gokan_outside/4gokan_outside_0_img_000180.png\",\n",
    "    \"../data/insta_frames/5gokan-denkisogokan/5gokan-denkisogokan_1_img_000010.png\",\n",
    "    \"../data/insta_frames/5gokan-mediacenter/5gokan-mediacenter_0_img_000030.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_1F/5gokan_1F_0_img_000070.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_2F/5gokan_2F_2_img_000030.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_3F/5gokan_3F_0_img_000040.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_ent_east/5gokan_ent_east_2_img_000000.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_lounge/5gokan_lounge_2_img_000060.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_out_east/5gokan_out_east_0_img_000270.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_parking/5gokan_parking_1_img_000170.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_smoking/5gokan_smoking_0_img_000270.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_stairs_cnt/5gokan_stairs_cnt_3_img_000010.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_stairs_west/5gokan_stairs_west_0_img_000340.png\",\n",
    "    \"../data/insta_frames/8gokan/8gokan_1F/8gokan_1F_1_img_000000.png\",\n",
    "    \"../data/insta_frames/8gokan/8gokan_ent_north/8gokan_ent_north_0_img_000010.png\",\n",
    "    \"../data/insta_frames/8gokan/8gokan_ent_south/8gokan_ent_south_1_img_000070.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_2F/denkisogokan_2F_2_img_000300.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_3F/denkisogokan_3F_0_img_000010.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_4F/denkisogokan_4F_0_img_000170.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_elevator/denkisogokan_elevator_0_img_000220.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_lounge/denkisogokan_lounge_1_img_000060.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_stairs/denkisogokan_stairs_0_img_000120.png\",\n",
    "    \"../data/insta_frames/lab-lounge/lab-lounge_0_img_000170.png\",\n",
    "    \"../data/insta_frames/lab/lab_bs_cnt/lab_bs_cnt_0_img_000270.png\",\n",
    "    \"../data/insta_frames/lab/lab_corner/lab_corner_0_img_000270.png\",\n",
    "    \"../data/insta_frames/lab/lab_desk/lab_desk_8_img_000210.png\",\n",
    "    \"../data/insta_frames/lab/lab_desk_table/lab_desk_table_3_img_000150.png\",\n",
    "    \"../data/insta_frames/lab/lab_ent/lab_ent_0_img_000020.png\",\n",
    "    \"../data/insta_frames/lab/lab_printer/lab_printer_0_img_000030.png\",\n",
    "    \"../data/insta_frames/lab/lab_table/lab_table_0_img_000170.png\",\n",
    "    \"../data/insta_frames/lab/lab_wb_cnt/lab_wb_cnt_0_img_000120.png\",\n",
    "    \"../data/insta_frames/lab/lab_wb_ent/lab_wb_ent_1_img_000160.png\",\n",
    "    \"../data/insta_frames/lounge/lounge_1_img_000060.png\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder() \n",
    "le.fit(labs.s_class) \n",
    "labs[\"labels\"] = le.transform(labs.s_class) \n",
    "labs.groupby('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_table = labs.groupby('labels').s_class.unique() \n",
    "cor_table = pd.DataFrame(cor_table) \n",
    "cor_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_table.to_csv('cor_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = labs.drop(['s_class'], axis=1)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainデータ, testデータの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dfs, test_size=0.2, random_state=42, stratify=dfs.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画像の前処理を定義\n",
    "data_transforms = {\n",
    "    'data': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "    ])\n",
    "}\n",
    "#正規化をしない処理\n",
    "to_tensor_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "        \n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        #前処理クラスの指定\n",
    "        self.transform = transform\n",
    "        #pandasでcsvデータの読み出し\n",
    "        #画像とラベルの一覧を保持するリスト\n",
    "        self.images = np.array(dataframe.img_path).tolist()\n",
    "        self.labels = np.array(dataframe.labels).tolist()\n",
    "        self.root_dir = root_dir\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #dataframeから画像へのパスとラベルを読み出す\n",
    "        label = self.labels[idx]\n",
    "        img = self.images[idx]\n",
    "        #画像の読み込み\n",
    "        with open(img, 'rb') as f:\n",
    "            image = Image.open(f)\n",
    "            image = image.convert('RGB')\n",
    "            image = image.resize((224,224))\n",
    "        #画像への処理\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(dataframe=train_data, root_dir=\"../data/insta_frames\", transform=data_transforms['data'])\n",
    "test_set = CustomDataset(dataframe=test_data, root_dir=\"../data/insta_frames\", transform=data_transforms['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderのcollate_fnはバッチ内のtensorのshapeをすべて同じにする必要がある\n",
    "# 自分で指定してエラーが起きないようにする\n",
    "def my_collate_fn(batch):\n",
    "    # datasetの出力が\n",
    "    # [image, target] = dataset[batch_idx]\n",
    "    # の場合.\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image, label in batch:\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    images = torch.stack(images,dim=0)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=16, shuffle=True, num_workers=6)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=16, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワークの定義\n",
    "vgg16 finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from spherenet import SphereConv2D, SphereMaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = num_labs_class\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = models.vgg16(pretrained=True, progress=True)\n",
    "net.features[0] = SphereConv2D(3, 64, stride=1)\n",
    "net.features[2] = SphereConv2D(64, 64, stride=1)\n",
    "net.features[5] = SphereConv2D(64, 128, stride=1)\n",
    "net.features[7] = SphereConv2D(128, 128, stride=1)\n",
    "net.features[10] = SphereConv2D(128, 256, stride=1)\n",
    "net.features[12] = SphereConv2D(256, 256, stride=1)\n",
    "net.features[14] = SphereConv2D(256, 256, stride=1)\n",
    "net.features[17] = SphereConv2D(256, 512, stride=1)\n",
    "net.features[19] = SphereConv2D(512, 512, stride=1)\n",
    "net.features[21] = SphereConv2D(512, 512, stride=1)\n",
    "net.features[24] = SphereConv2D(512, 512, stride=1)\n",
    "net.features[26] = SphereConv2D(512, 512, stride=1)\n",
    "net.features[28] = SphereConv2D(512, 512, stride=1)\n",
    "net.features[4] = SphereMaxPool2D(stride=2)\n",
    "net.features[9] = SphereMaxPool2D(stride=2)\n",
    "net.features[16] = SphereMaxPool2D(stride=2)\n",
    "net.features[23] = SphereMaxPool2D(stride=2)\n",
    "net.features[30] = SphereMaxPool2D(stride=2)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パラメータ凍結と採取層クラス数変更\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "#最終層をnum_s_classクラス用に変, lr=args.lr更\n",
    "num_ftrs = net.classifier[6].in_features\n",
    "net.classifier[6] = nn.Linear(in_features=num_ftrs, out_features=num_classes).to(device)\n",
    "#最適化関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.classifier[6].parameters(), lr=1e-4)\n",
    "net = net.to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#学習率の変更\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validationを行いたい...\n",
    "#Early Stopping を行いたい\n",
    "\n",
    "num_epochs = 10\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    #train\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #view()での変換をしない\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if images.dim() == 3:\n",
    "                images = images.unsqueeze(1)  # (B, H, W) -> (B, C, H, W)\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (outputs.max(1)[1]==labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_train_loss = train_loss/len(train_loader.dataset)\n",
    "    avg_train_acc = train_acc/len(train_loader.dataset)\n",
    "    \n",
    "    #val\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            #view()での変換をしない\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            if images.dim() == 3:\n",
    "                images = .unsqueeze(1)  # (B, H, W) -> (B, C, H, W)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (outputs.max(1, keepdim=True)[1]==labels).sum().item()\n",
    "   \n",
    "    avg_val_loss = val_loss/len(test_loader.dataset)\n",
    "    avg_val_acc = val_acc/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {loss:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}'.format(epoch+1, num_epochs, i+1, loss=avg_train_loss, val_loss=avg_val_loss, val_acc=avg_val_acc))\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    train_acc_list.append(avg_train_acc)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    val_acc_list.append(avg_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, validationのloss acc のグラフを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(num_epochs), train_loss_list, color='blue', linestyle='-', label='train_loss')\n",
    "plt.plot(range(num_epochs), val_loss_list, color='green', linestyle='--', label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), train_acc_list, color='blue', linestyle='-', label='train_acc')\n",
    "plt.plot(range(num_epochs), val_acc_list, color='green', linestyle='--', label='test_acc')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.title('Training and validation acc')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAMの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCam:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.eval()\n",
    "        self.feature = None\n",
    "        self.gradient = None\n",
    "    \n",
    "    def save_gradient(self, grad):\n",
    "        self.gradient = grad\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        image_size = (x.size(-1), x.size(-2))\n",
    "        feature_maps =[]\n",
    "        \n",
    "        for i in range(x.size(0)):\n",
    "            img = x[i].data.cpu().numpy() #GPU上のTensorはcpuに移さないとnumpyに変換できない\n",
    "            img = img - np.min(img)\n",
    "            if np.max(img) != 0:\n",
    "                img = img / np.max(img)\n",
    "            \n",
    "            feature = x[i].unsqueeze(0)\n",
    "            \n",
    "            for name, module in self.model.named_children():\n",
    "                if name == 'clasifier':\n",
    "                    feature = feature.view(feature.size(0), -1)\n",
    "                feature = module(feature)\n",
    "                if name == 'features':\n",
    "                    feature.register_hook(self.save_gradient)\n",
    "                    self.feature = feature\n",
    "                    \n",
    "            classes = F.sigmoid(feature)\n",
    "            one_hot, _ = classes.max(dim=-1)\n",
    "            self.model.zero_grad()\n",
    "            one_hot.backward()\n",
    "            \n",
    "            weight = self.gradient.mean(dim=-1, keepdim=True).mean(dim=-2, keepdim=True)\n",
    "            \n",
    "            mask = F.relu((weight*self.feature).sum(dim=1)).squeeze(0)\n",
    "            mask = cv2.resize(mask.data.cpu().numpy(), image_size)\n",
    "            mask = mask - np.min(mask)\n",
    "            \n",
    "            if np.max(mask) != 0:\n",
    "                mask = mask/np.max(mask)\n",
    "                \n",
    "            feature_map = np.float32(cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET))\n",
    "            cam = feature_map + np.float32((np.uint8(img.transpose((1,2,0))*225)))\n",
    "            cam = cam - np.min(cam)\n",
    "            \n",
    "            if np.max(cam) != 0:\n",
    "                cam = cam/np.max(cam)\n",
    "                \n",
    "            feature_maps.append(transforms.ToTensor()(cv2.cvtColor(np.uint8(225*cam), cv2.COLOR_BGR2RGB)))\n",
    "            \n",
    "        feature_maps = torch.stack(feature_maps)\n",
    "        \n",
    "        return feature_maps\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cam_test_img_path)):\n",
    "    #入力画像の読み込み\n",
    "    cam_test_img = Image.open(cam_test_img_path[i])\n",
    "    cam_img_tensor = (data_transforms['data']((cam_test_img))).unsqueeze(dim=0)\n",
    "    \n",
    "    cam_img_tensor = cam_img_tensor.to(device)\n",
    "    \n",
    "    img_size = cam_test_img.size\n",
    "    #grad-camによる予測根拠可視化\n",
    "    gradcam = GradCam(net)\n",
    "    \n",
    "    feature_image = gradcam(cam_img_tensor).squeeze(dim=0)\n",
    "    feature_image = transforms.ToPILImage()(feature_image)\n",
    "    \n",
    "    pred_idx = net(cam_img_tensor).max(1)[1]\n",
    "                      \n",
    "    save_dir = '../data/gradcam_img/VGG16/'+s_classlist[i]\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\t# Make a directory\n",
    "    #保存先ディレクトリ名はその画像のクラス，画像の予測値を画像の名前に書き込む\n",
    "    cv2.imwrite(save_dir+'/heatmap_pred_'+s_classlist[pred_idx]+'.jpg', superimposed_img)\n",
    "    print('Saved: ', save_dir+'/heatmap_pred_'+s_classlist[pred_idx]+'.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
