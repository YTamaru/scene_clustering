{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skorch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/'\n",
    "csv_path = os.path.join(DATA_FOLDER, 'frames_data.csv')\n",
    "datalist = pd.read_csv(os.path.join(DATA_FOLDER, 'frames_data.csv'), names=[\"img_path\", \"l_class\", 's_class','timestamp'])\n",
    "datalist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = datalist.drop(['s_class','timestamp'], axis=1)\n",
    "dfl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.groupby('l_class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = datalist.drop(['l_class', 'timestamp'], axis=1)\n",
    "dfs.groupby('s_class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.img_path.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = dfs[dfs['s_class'].str.startswith('lab')]\n",
    "num_labs_class = 10\n",
    "labs.s_class.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder() \n",
    "le.fit(labs.s_class) \n",
    "labs[\"labels\"] = le.transform(labs.s_class) \n",
    "labs.groupby('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_table = labs.groupby('labels').s_class.unique() \n",
    "cor_table = pd.DataFrame(cor_table) \n",
    "cor_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_table.to_csv('cor_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = labs.drop(['s_class'], axis=1)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainデータ, testデータの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dfs, test_size=0.2, random_state=42, stratify=dfs.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画像の前処理を定義\n",
    "data_transforms = {\n",
    "    'data': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "    ])\n",
    "}\n",
    "#正規化をしない処理\n",
    "to_tensor_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "        \n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        #前処理クラスの指定\n",
    "        self.transform = transform\n",
    "        #pandasでcsvデータの読み出し\n",
    "        #画像とラベルの一覧を保持するリスト\n",
    "        self.images = np.array(dataframe.img_path).tolist()\n",
    "        self.labels = np.array(dataframe.labels).tolist()\n",
    "        self.root_dir = root_dir\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #dataframeから画像へのパスとラベルを読み出す\n",
    "        label = self.labels[idx]\n",
    "        img = self.images[idx]\n",
    "        #画像の読み込み\n",
    "        with open(img, 'rb') as f:\n",
    "            image = Image.open(f)\n",
    "            image = image.convert('RGB')\n",
    "            image = image.resize((224,224))\n",
    "        #画像への処理\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(dataframe=train_data, root_dir=\"../data/insta_frames\", transform=data_transforms['data'])\n",
    "test_set = CustomDataset(dataframe=test_data, root_dir=\"../data/insta_frames\", transform=data_transforms['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderのcollate_fnはバッチ内のtensorのshapeをすべて同じにする必要がある\n",
    "# 自分で指定してエラーが起きないようにする\n",
    "def my_collate_fn(batch):\n",
    "    # datasetの出力が\n",
    "    # [image, target] = dataset[batch_idx]\n",
    "    # の場合.\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image, label in batch:\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    images = torch.stack(images,dim=0)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=16, shuffle=True, num_workers=6)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=16, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワークの定義\n",
    "vgg16 finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from s2cnn import SO3Convolution\n",
    "from s2cnn import S2Convolution\n",
    "from s2cnn import so3_integrate\n",
    "from s2cnn import so3_near_identity_grid\n",
    "from s2cnn import s2_near_identity_grid\n",
    "import gzip\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = num_labs_class\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2ConvNet_deep(nn.Module):\n",
    "\n",
    "    def __init__(self, bandwidth=30):\n",
    "        super(S2ConvNet_deep, self).__init__()\n",
    "\n",
    "        grid_s2    =  s2_near_identity_grid(n_alpha=6, max_beta=np.pi/16, n_beta=1)\n",
    "        grid_so3_1 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/16, n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "        grid_so3_2 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/ 8, n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "        grid_so3_3 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/ 4, n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "        grid_so3_4 = so3_near_identity_grid(n_alpha=6, max_beta=np.pi/ 2, n_beta=1, max_gamma=2*np.pi, n_gamma=6)\n",
    "\n",
    "        self.convolutional = nn.Sequential(\n",
    "            S2Convolution(\n",
    "                nfeature_in  = 3,\n",
    "                nfeature_out = 64,\n",
    "                b_in  = bandwidth,\n",
    "                b_out = bandwidth,\n",
    "                grid=grid_s2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  =  64,\n",
    "                nfeature_out = 64,\n",
    "                b_in  = bandwidth,\n",
    "                b_out = bandwidth//2,\n",
    "                grid=grid_so3_1),\n",
    "            nn.ReLU(inplace=False),\n",
    "\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 64,\n",
    "                nfeature_out = 128,\n",
    "                b_in  = bandwidth//2,\n",
    "                b_out = bandwidth//2,\n",
    "                grid=grid_so3_2),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 128,\n",
    "                nfeature_out = 128,\n",
    "                b_in  = bandwidth//2,\n",
    "                b_out = bandwidth//4,\n",
    "                grid=grid_so3_2),\n",
    "            nn.ReLU(inplace=False),\n",
    "\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 128,\n",
    "                nfeature_out = 256,\n",
    "                b_in  = bandwidth//4,\n",
    "                b_out = bandwidth//4,\n",
    "                grid=grid_so3_3),\n",
    "            nn.ReLU(inplace=False),\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 256,\n",
    "                nfeature_out = 256,\n",
    "                b_in  = bandwidth//4,\n",
    "                b_out = bandwidth//8,\n",
    "                grid=grid_so3_3),\n",
    "            nn.ReLU(inplace=False),\n",
    "\n",
    "            SO3Convolution(\n",
    "                nfeature_in  = 256,\n",
    "                nfeature_out = 256,\n",
    "                b_in  = bandwidth//8,\n",
    "                b_out = bandwidth//8,\n",
    "                grid=grid_so3_4),\n",
    "            nn.ReLU(inplace=False)\n",
    "            )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            # linear 1\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(in_features=64,out_features=64),\n",
    "            nn.ReLU(inplace=False),\n",
    "            # linear 2\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.ReLU(inplace=False),\n",
    "            # linear 3\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Linear(in_features=32, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutional(x)\n",
    "        x = so3_integrate(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = S2ConvNet_deep()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パラメータ凍結と採取層クラス数変更\n",
    "# for param in net.parameters():\n",
    "#     param.requires_grad = False\n",
    "#最終層をnum_s_classクラス用に変, lr=args.lr更\n",
    "# num_ftrs = net.classifier[6].in_features\n",
    "# net.classifier[6] = nn.Linear(in_features=num_ftrs, out_features=num_classes).to(device)\n",
    "#最適化関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-3)\n",
    "net = net.to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#学習率の変更\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validationを行いたい...\n",
    "#Early Stopping を行いたい\n",
    "\n",
    "num_epochs = 10\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    \n",
    "    #train\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        net.train() #?\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (outputs.max(1)[1]==labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_train_loss = train_loss/len(train_loader.dataset)\n",
    "    avg_train_acc = train_acc/len(train_loader.dataset)\n",
    "    \n",
    "    #val\n",
    "    for images, labels in test_loader:\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (outputs.max(1)[1]==labels).sum().item()\n",
    "   \n",
    "    avg_val_loss = val_loss/len(test_loader.dataset)\n",
    "    avg_val_acc = val_acc/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {loss:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}'.format(epoch+1, num_epochs, i+1, loss=avg_train_loss, val_loss=avg_val_loss, val_acc=avg_val_acc))\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    train_acc_list.append(avg_train_acc)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    val_acc_list.append(avg_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, validationのloss acc のグラフを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(num_epochs), train_loss_list, color='blue', linestyle='-', label='train_loss')\n",
    "plt.plot(range(num_epochs), val_loss_list, color='green', linestyle='--', label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), train_acc_list, color='blue', linestyle='-', label='train_acc')\n",
    "plt.plot(range(num_epochs), val_acc_list, color='green', linestyle='--', label='test_acc')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.title('Training and validation acc')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAMの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCam:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.eval()\n",
    "        self.feature = None\n",
    "        self.gradient = None\n",
    "    \n",
    "    def save_gradient(self, grad):\n",
    "        self.gradient = grad\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        image_size = (x.size(-1), x.size(-2))\n",
    "        feature_maps =[]\n",
    "        \n",
    "        for i in range(x.size(0)):\n",
    "            img = x[i].data.cpu().numpy() #GPU上のTensorはcpuに移さないとnumpyに変換できない\n",
    "            img = img - np.min(img)\n",
    "            if np.max(img) != 0:\n",
    "                img = img / np.max(img)\n",
    "            \n",
    "            feature = x[i].unsqueeze(0)\n",
    "            \n",
    "            for name, module in self.model.named_children():\n",
    "                if name == 'clasifier':\n",
    "                    feature = feature.view(feature.size(0), -1)\n",
    "                feature = module(feature)\n",
    "                if name == 'features':\n",
    "                    feature.register_hook(self.save_gradient)\n",
    "                    self.feature = feature\n",
    "                    \n",
    "            classes = F.sigmoid(feature)\n",
    "            one_hot, _ = classes.max(dim=-1)\n",
    "            self.model.zero_grad()\n",
    "            one_hot.backward()\n",
    "            \n",
    "            weight = self.gradient.mean(dim=-1, keepdim=True).mean(dim=-2, keepdim=True)\n",
    "            \n",
    "            mask = F.relu((weight*self.feature).sum(dim=1)).squeeze(0)\n",
    "            mask = cv2.resize(mask.data.cpu().numpy(), image_size)\n",
    "            mask = mask - np.min(mask)\n",
    "            \n",
    "            if np.max(mask) != 0:\n",
    "                mask = mask/np.max(mask)\n",
    "                \n",
    "            feature_map = np.float32(cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET))\n",
    "            cam = feature_map + np.float32((np.uint8(img.transpose((1,2,0))*225)))\n",
    "            cam = cam - np.min(cam)\n",
    "            \n",
    "            if np.max(cam) != 0:\n",
    "                cam = cam/np.max(cam)\n",
    "                \n",
    "            feature_maps.append(transforms.ToTensor()(cv2.cvtColor(np.uint8(225*cam), cv2.COLOR_BGR2RGB)))\n",
    "            \n",
    "        feature_maps = torch.stack(feature_maps)\n",
    "        \n",
    "        return feature_maps\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cam_test_img_path)):\n",
    "    #入力画像の読み込み\n",
    "    cam_test_img = Image.open(cam_test_img_path[i])\n",
    "    cam_img_tensor = (data_transforms['data']((cam_test_img))).unsqueeze(dim=0)\n",
    "    \n",
    "    cam_img_tensor = cam_img_tensor.to(device)\n",
    "    \n",
    "    img_size = cam_test_img.size\n",
    "    #grad-camによる予測根拠可視化\n",
    "    gradcam = GradCam(net)\n",
    "    \n",
    "    feature_image = gradcam(cam_img_tensor).squeeze(dim=0)\n",
    "    feature_image = transforms.ToPILImage()(feature_image)\n",
    "    \n",
    "    pred_idx = net(cam_img_tensor).max(1)[1]\n",
    "                      \n",
    "    save_dir = '../data/gradcam_img/VGG16/'+s_classlist[i]\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\t# Make a directory\n",
    "    #保存先ディレクトリ名はその画像のクラス，画像の予測値を画像の名前に書き込む\n",
    "    cv2.imwrite(save_dir+'/heatmap_pred_'+s_classlist[pred_idx]+'.jpg', superimposed_img)\n",
    "    print('Saved: ', save_dir+'/heatmap_pred_'+s_classlist[pred_idx]+'.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
