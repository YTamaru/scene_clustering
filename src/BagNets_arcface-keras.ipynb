{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random as rn\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflowの乱数シード固定(再現性の担保)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(7)\n",
    "rn.seed(7)\n",
    "\n",
    "session_conf = tf.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")\n",
    "\n",
    "tf.set_random_seed(7)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像データ取り込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = ''\n",
    "datalist = pd.read_csv(os.path.join(DATA_FOLDER, ''), names=[\"img_path\", \"l_class\", 's_class','timestamp'])\n",
    "datalist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = datalist.drop(['l_class', 'timestamp'], axis=1)\n",
    "dfs.groupby('s_class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.img_path.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_s_class = len(dfs.s_class.unique())\n",
    "num_s_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown\n",
    "le = LabelEncoder()\n",
    "le.fit(dfs.s_class)\n",
    "dfs[\"labels\"] = le.transform(dfs.s_class)\n",
    "dfs.groupby('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown\n",
    "cor_table = dfs.groupby('labels').s_class.unique()\n",
    "cor_table = pd.DataFrame(cor_table)\n",
    "cor_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown\n",
    "dfs = dfs.drop(['s_class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown\n",
    "x = []\n",
    "for i in range(len(dfs.img_path)):\n",
    "    image = img_to_array(load_img(dfs.img_path[i], target_size=(224,224)))\n",
    "    x.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown\n",
    "y = dfs.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown\n",
    "#正規化\n",
    "x = np.array(x, dtype='float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test & validation set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_valid, y, y_valid = train_test_split(x,y, random_state=42, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA(探索的データ解析)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,18))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05, wspace=0.05)\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(1, 5, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BagNets17　Using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Activation, add, Add, Dropout, BatchNormalization, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bagnets.kerasnet\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "from scheduler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg_model = bagnets.kerasnet.bagnet17()\n",
    "vgg_model = VGG16(weights=None, include_top=False, input_shape=(224,224,3))\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vgg_model.output\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, kernel_initializer='he_normal',\n",
    "            kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "x = BatchNormalization()(x)\n",
    "output = ArcFace(num_s_class, regularizer=regularizers.l2(weight_decay))([x, y])\n",
    "full_model = Model(inputs=vgg_model.input, outputs=prediction)\n",
    "\n",
    "for layer in full_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose=1, mode='auto')\n",
    "full_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 層化抽出法(Stratified sampling)&KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=42)\n",
    "cvscores = []\n",
    "#Stratified KFoldはlabel encoding化した整数ベクトルでないともちいることができない\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    #One-hot化　CNNに入出力できるように整形\n",
    "    y_train = np_utils.to_categorical(y_train, num_s_class)\n",
    "    y_test = np_utils.to_categorical(y_test, num_s_class)\n",
    "    full_model.fit(x_train, y_train, verbose=1, batch_size=batch_size, epochs=n_epochs,\n",
    "              callbacks=[early_stopping], validation_split=0.2)\n",
    "    scores = full_model.evaluate(x_test, y_test)\n",
    "    print(\"%s: %.2f%%\" % (full_model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = full_model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = np_utils.to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_valid)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse.csgraph import connected_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmx = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(cmx, annot=True)\n",
    "plt.show()\n",
    "print('accuracy: {}'.format(accuracy_score\n",
    "(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_table = pd.read_csv('cor_table.csv')\n",
    "print(cor_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = []\n",
    "for i in range(num_s_class):\n",
    "    true.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "\n",
    "grad_last_conv = full_model.get_layer(\"block5_conv3\")\n",
    "model_output = full_model.output[:,0]\n",
    "grads = K.gradients(model_output, grad_last_conv.output)[0]\n",
    "pooled_grads = K.mean(grads, axis=(0,1,2))\n",
    "iterate = K.function([full_model.input], [pooled_grads, grad_last_conv.output[0]])\n",
    "\n",
    "for n in range(len(grad_lab_img)):\n",
    "    img_keras = image.load_img(grad_lab_img[n], target_size=(224, 224))\n",
    "    img_tensor = image.img_to_array(img_keras)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "    predicts = full_model.predict(img_tensor)\n",
    "    s_class_num = np.argmax(predicts)\n",
    "    pred.append(s_class_num)\n",
    "    img_tensor /= 255.\n",
    "    pooled_grads_val, conv_output_val = iterate([img_tensor])\n",
    "    for i in range(pooled_grads_val.shape[0]):\n",
    "        conv_output_val[:, :, i] *= pooled_grads_val[i]\n",
    "    heatmap = np.mean(conv_output_val, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    img = cv2.imread(grad_lab_img[n])\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * 0.4 + img\n",
    "    save_dir = '../data/gradcam_img/VGG16'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\t# Make a directory\n",
    "    #保存先ディレクトリ名はその画像のクラス，画像の予測値を画像の名前に書き込む\n",
    "    cv2.imwrite(save_dir+'/heatmap_wrong_pred_'+s_classlist[s_class_num]+'_'+str(n)+'_'+'.jpg', superimposed_img)\n",
    "    print('Saved: ', save_dir+'/heatmap_wrong_pred_'+s_classlist[s_class_num]+'.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def grad_cam(input_model, image, cls, layer_name):\n",
    "    \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n",
    "    y_c = input_model.output[0, cls]\n",
    "    conv_output = input_model.get_layer(layer_name).output\n",
    "    grads = K.gradients(y_c, conv_output)[0]\n",
    "    # Normalize if necessary\n",
    "    # grads = normalize(grads)\n",
    "    gradient_function = K.function([input_model.input], [conv_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "\n",
    "    weights = np.mean(grads_val, axis=(0, 1))\n",
    "    cam = np.dot(output, weights)\n",
    "\n",
    "    # Process CAM\n",
    "    cam = cv2.resize(cam, (W, H), cv2.INTER_LINEAR)\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam_max = cam.max() \n",
    "    if cam_max != 0: \n",
    "        cam = cam / cam_max\n",
    "    return cam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for n in range(len(grad_lab_img)):\n",
    "    cam = grad_cam(input_model=full_model, image=grad_lab_img[n], cls=-1, layer_name=\"block5_conv3\")\n",
    "    save_dir = '../data/gradcam_test/VGG16'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\t# Make a directory\n",
    "    #保存先ディレクトリ名はその画像のクラス，画像の予測値を画像の名前に書き込む\n",
    "    cv2.imwrite(save_dir+'/heatmap_wrong_pred_'+s_classlist[s_class_num]+'_'+str(n)+'_'+'.jpg', cam)\n",
    "    print('Saved: ', save_dir+'/heatmap_wrong_pred_'+s_classlist[s_class_num]+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grad = pd.DataFrame(grad_lab_img)\n",
    "df_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmx = confusion_matrix(true, pred)\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(cmx, annot=True)\n",
    "plt.ylabel(\"truth\")\n",
    "plt.xlabel(\"prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_table = pd.read_csv('cor_table.csv')\n",
    "print(cor_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction->edge ==> padding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
