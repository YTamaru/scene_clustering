{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skorch\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "l_classlist = ['4gokan', '5gokan', '5gokan-denkisogokan', '5gokan-mediacenter'\n",
    "                    '8gokan', 'denkisogokan', 'lab', 'lab-lounge', 'lounge']\n",
    "s_classlist = ['4gokan_inside', '4gokan_outside', '5gokan-denkisogokan', '5gokan-mediacenter',\n",
    "'5gokan_1F', '5gokan_2F', '5gokan_3F', '5gokan_ent_east', '5gokan_lounge',\n",
    "'5gokan_out_east', '5gokan_parking', '5gokan_smoking', '5gokan_stairs_cnt',\n",
    "'5gokan_stairs_west', '8gokan_1F', '8gokan_ent_north', '8gokan_ent_south',\n",
    "'denkisogokan_2F', 'denkisogokan_3F', 'denkisogokan_4F', 'denkisogokan_elevator',\n",
    "'denkisogokan_lounge', 'denkisogokan_stairs', 'lab-lounge', 'lab_bs_cnt',\n",
    "'lab_corner', 'lab_desk', 'lab_desk_table', 'lab_ent', 'lab_printer',\n",
    "'lab_table', 'lab_wb_cnt', 'lab_wb_ent', 'lounge']\n",
    "num_l_class =len(l_classlist)\n",
    "num_s_class = len(s_classlist)\n",
    "print(num_s_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>l_class</th>\n",
       "      <th>s_class</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "      <td>2019/10/23 18:50:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "      <td>2019/10/23 18:50:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "      <td>2019/10/23 18:50:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "      <td>2019/10/23 18:50:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "      <td>2019/10/23 18:50:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path              l_class  \\\n",
       "0  ../data/insta_frames/5gokan-denkisogokan/5goka...  5gokan-denkisogokan   \n",
       "1  ../data/insta_frames/5gokan-denkisogokan/5goka...  5gokan-denkisogokan   \n",
       "2  ../data/insta_frames/5gokan-denkisogokan/5goka...  5gokan-denkisogokan   \n",
       "3  ../data/insta_frames/5gokan-denkisogokan/5goka...  5gokan-denkisogokan   \n",
       "4  ../data/insta_frames/5gokan-denkisogokan/5goka...  5gokan-denkisogokan   \n",
       "\n",
       "               s_class            timestamp  \n",
       "0  5gokan-denkisogokan  2019/10/23 18:50:43  \n",
       "1  5gokan-denkisogokan  2019/10/23 18:50:43  \n",
       "2  5gokan-denkisogokan  2019/10/23 18:50:43  \n",
       "3  5gokan-denkisogokan  2019/10/23 18:50:43  \n",
       "4  5gokan-denkisogokan  2019/10/23 18:50:43  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FOLDER = '../data/'\n",
    "csv_path = os.path.join(DATA_FOLDER, 'frames_data.csv')\n",
    "datalist = pd.read_csv(os.path.join(DATA_FOLDER, 'frames_data.csv'), names=[\"img_path\", \"l_class\", 's_class','timestamp'])\n",
    "datalist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>l_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>5gokan-denkisogokan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path              l_class\n",
       "0  ../data/insta_frames/5gokan-denkisogokan/5goka...  5gokan-denkisogokan\n",
       "1  ../data/insta_frames/5gokan-denkisogokan/5goka...  5gokan-denkisogokan\n",
       "2  ../data/insta_frames/5gokan-denkisogokan/5goka...  5gokan-denkisogokan\n",
       "3  ../data/insta_frames/5gokan-denkisogokan/5goka...  5gokan-denkisogokan\n",
       "4  ../data/insta_frames/5gokan-denkisogokan/5goka...  5gokan-denkisogokan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfl = datalist.drop(['s_class','timestamp'], axis=1)\n",
    "dfl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l_class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4gokan</th>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan</th>\n",
       "      <td>1328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan-denkisogokan</th>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan-mediacenter</th>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8gokan</th>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denkisogokan</th>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab</th>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab-lounge</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lounge</th>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     img_path\n",
       "l_class                      \n",
       "4gokan                    308\n",
       "5gokan                   1328\n",
       "5gokan-denkisogokan       361\n",
       "5gokan-mediacenter        127\n",
       "8gokan                    266\n",
       "denkisogokan              523\n",
       "lab                      1953\n",
       "lab-lounge                 63\n",
       "lounge                    247"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfl.groupby('l_class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s_class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4gokan_inside</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4gokan_outside</th>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan-denkisogokan</th>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan-mediacenter</th>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan_1F</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan_2F</th>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan_3F</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan_ent_east</th>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan_lounge</th>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan_out_east</th>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan_parking</th>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan_smoking</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan_stairs_cnt</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5gokan_stairs_west</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8gokan_1F</th>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8gokan_ent_north</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8gokan_ent_south</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denkisogokan_2F</th>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denkisogokan_3F</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denkisogokan_4F</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denkisogokan_elevator</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denkisogokan_lounge</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denkisogokan_stairs</th>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab-lounge</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab_bs_cnt</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab_corner</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab_desk</th>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab_desk_table</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab_ent</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab_printer</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab_table</th>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab_wb_cnt</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lab_wb_ent</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lounge</th>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       img_path\n",
       "s_class                        \n",
       "4gokan_inside                60\n",
       "4gokan_outside              248\n",
       "5gokan-denkisogokan         361\n",
       "5gokan-mediacenter          127\n",
       "5gokan_1F                    65\n",
       "5gokan_2F                    96\n",
       "5gokan_3F                    32\n",
       "5gokan_ent_east             135\n",
       "5gokan_lounge               361\n",
       "5gokan_out_east              99\n",
       "5gokan_parking              251\n",
       "5gokan_smoking               65\n",
       "5gokan_stairs_cnt           187\n",
       "5gokan_stairs_west           37\n",
       "8gokan_1F                   208\n",
       "8gokan_ent_north             26\n",
       "8gokan_ent_south             32\n",
       "denkisogokan_2F             120\n",
       "denkisogokan_3F              33\n",
       "denkisogokan_4F              31\n",
       "denkisogokan_elevator        30\n",
       "denkisogokan_lounge         198\n",
       "denkisogokan_stairs         111\n",
       "lab-lounge                   63\n",
       "lab_bs_cnt                   35\n",
       "lab_corner                   58\n",
       "lab_desk                    813\n",
       "lab_desk_table              137\n",
       "lab_ent                      30\n",
       "lab_printer                  31\n",
       "lab_table                   755\n",
       "lab_wb_cnt                   31\n",
       "lab_wb_ent                   63\n",
       "lounge                      247"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = datalist.drop(['l_class', 'timestamp'], axis=1)\n",
    "dfs.groupby('s_class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5176"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.img_path.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_test_img_path =[\n",
    "    \"../data/insta_frames/4gokan/4gokan_inside/4gokan_inside_0_img_000240.png\",\n",
    "    \"../data/insta_frames/4gokan/4gokan_outside/4gokan_outside_0_img_000180.png\",\n",
    "    \"../data/insta_frames/5gokan-denkisogokan/5gokan-denkisogokan_1_img_000010.png\",\n",
    "    \"../data/insta_frames/5gokan-mediacenter/5gokan-mediacenter_0_img_000030.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_1F/5gokan_1F_0_img_000070.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_2F/5gokan_2F_2_img_000030.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_3F/5gokan_3F_0_img_000040.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_ent_east/5gokan_ent_east_2_img_000000.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_lounge/5gokan_lounge_2_img_000060.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_out_east/5gokan_out_east_0_img_000270.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_parking/5gokan_parking_1_img_000170.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_smoking/5gokan_smoking_0_img_000270.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_stairs_cnt/5gokan_stairs_cnt_3_img_000010.png\",\n",
    "    \"../data/insta_frames/5gokan/5gokan_stairs_west/5gokan_stairs_west_0_img_000340.png\",\n",
    "    \"../data/insta_frames/8gokan/8gokan_1F/8gokan_1F_1_img_000000.png\",\n",
    "    \"../data/insta_frames/8gokan/8gokan_ent_north/8gokan_ent_north_0_img_000010.png\",\n",
    "    \"../data/insta_frames/8gokan/8gokan_ent_south/8gokan_ent_south_1_img_000070.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_2F/denkisogokan_2F_2_img_000300.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_3F/denkisogokan_3F_0_img_000010.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_4F/denkisogokan_4F_0_img_000170.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_elevator/denkisogokan_elevator_0_img_000220.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_lounge/denkisogokan_lounge_1_img_000060.png\",\n",
    "    \"../data/insta_frames/denkisogokan/denkisogokan_stairs/denkisogokan_stairs_0_img_000120.png\",\n",
    "    \"../data/insta_frames/lab-lounge/lab-lounge_0_img_000170.png\",\n",
    "    \"../data/insta_frames/lab/lab_bs_cnt/lab_bs_cnt_0_img_000270.png\",\n",
    "    \"../data/insta_frames/lab/lab_corner/lab_corner_0_img_000270.png\",\n",
    "    \"../data/insta_frames/lab/lab_desk/lab_desk_8_img_000210.png\",\n",
    "    \"../data/insta_frames/lab/lab_desk_table/lab_desk_table_3_img_000150.png\",\n",
    "    \"../data/insta_frames/lab/lab_ent/lab_ent_0_img_000020.png\",\n",
    "    \"../data/insta_frames/lab/lab_printer/lab_printer_0_img_000030.png\",\n",
    "    \"../data/insta_frames/lab/lab_table/lab_table_0_img_000170.png\",\n",
    "    \"../data/insta_frames/lab/lab_wb_cnt/lab_wb_cnt_0_img_000120.png\",\n",
    "    \"../data/insta_frames/lab/lab_wb_ent/lab_wb_ent_1_img_000160.png\",\n",
    "    \"../data/insta_frames/lounge/lounge_1_img_000060.png\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f7a5f89e9b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder() \n",
    "le.fit(dfs.s_class) \n",
    "dfs[\"labels\"] = le.transform(dfs.s_class) \n",
    "dfs.groupby('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4gokan_inside]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4gokan_outside]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5gokan-denkisogokan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[5gokan-mediacenter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[5gokan_1F]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      s_class\n",
       "labels                       \n",
       "0             [4gokan_inside]\n",
       "1            [4gokan_outside]\n",
       "2       [5gokan-denkisogokan]\n",
       "3        [5gokan-mediacenter]\n",
       "4                 [5gokan_1F]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_table = dfs.groupby('labels').s_class.unique() \n",
    "cor_table = pd.DataFrame(cor_table) \n",
    "cor_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_table.to_csv('cor_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs.drop(['s_class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/insta_frames/5gokan-denkisogokan/5goka...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path  labels\n",
       "0  ../data/insta_frames/5gokan-denkisogokan/5goka...       2\n",
       "1  ../data/insta_frames/5gokan-denkisogokan/5goka...       2\n",
       "2  ../data/insta_frames/5gokan-denkisogokan/5goka...       2\n",
       "3  ../data/insta_frames/5gokan-denkisogokan/5goka...       2\n",
       "4  ../data/insta_frames/5gokan-denkisogokan/5goka...       2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画像の前処理を定義\n",
    "data_transforms = {\n",
    "    \"data\": transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "    ])\n",
    "}\n",
    "#正規化をしない処理\n",
    "to_tensor_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "        \n",
    "    def __init__(self, dataframe, transform=None, train=True):\n",
    "        #前処理クラスの指定\n",
    "        self.transform = transform\n",
    "        #画像とラベルの一覧を保持するリスト\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        #pandasでcsvデータの読み出し\n",
    "        self.image_dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #dataframeから画像へのパスとラベルを読み出す\n",
    "        label = self.image_dataframe.labels[idx]\n",
    "        img_path = self.image_dataframe.img_path[idx]\n",
    "        #画像の読み込み\n",
    "        image = Image.open(img_path)\n",
    "        #画像への処理\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgDataset = CustomDataset(dfs, transform=data_transforms['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomDataset object at 0x7f7a5f841198>\n"
     ]
    }
   ],
   "source": [
    "print(imgDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainデータ, testデータの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 層化抽出を行いたい...\n",
    "# 引数がクラスインスタンスのためインデックス参照ができずに，\n",
    "# sklearnではstratifyが使えない...\n",
    "train_data, test_data = train_test_split(imgDataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワークの定義\n",
    "vgg16 finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = num_s_class\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = models.vgg16(pretrained=True, progress=True)\n",
    "net = net.to(device)\n",
    "\n",
    "#最適化関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=34, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#パラメータ凍結と採取層クラス数変更\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "net = net.to(device)\n",
    "#最終層をnum_s_classクラス用に変更\n",
    "num_ftrs = net.classifier[6].in_features\n",
    "net.classifier[6] = nn.Linear(num_ftrs, num_s_class)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#学習率の変更\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 448 and 398 in dimension 3 at /pytorch/aten/src/TH/generic/THTensor.cpp:689",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-99a66c703d4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m#view()での変換をしない\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 448 and 398 in dimension 3 at /pytorch/aten/src/TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "#Cross Validationを行いたい...\n",
    "\n",
    "num_epochs = 50\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    #train\n",
    "    net.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #view()での変換をしない\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (outputs.max(1)[1]==labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_train_loss = train_loss/len(train_loader.dataset)\n",
    "    avg_train_acc = train_acc/len(train_loader.dataset)\n",
    "    \n",
    "    #val\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            #view()での変換をしない\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (outputs.max(1)[1]==labels).sum().item()\n",
    "   \n",
    "    avg_val_loss = val_loss/len(test_loader.dataset)\n",
    "    avg_val_acc = val_acc/len(test_loader.dataset)\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {loss:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}'.format(epoch+1, num_epochs, i+1, loss=avg_train_loss, val_loss=avg_val_loss, val_acc=avg_val_acc))\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    train_acc_list.append(avg_train_acc)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    val_acc_list.append(avg_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train, validationのloss acc のグラフを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(num_epochs), train_loss_list, color='blue', linestyle='_', label='train_loss')\n",
    "plt.plot(range(num_epochs), val_loss_list, color='green', linestyle='__', label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(num_epochs), train_acc_list, color='blue', linestyle='_', label='train_acc')\n",
    "plt.plot(range(num_epochs), test_acc_list, color='green', linestyle='__', label='test_acc')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.title('Training and validation acc')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAMの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCam:\n",
    "    def __init__(self, model):\n",
    "        self.model = model.eval()\n",
    "        self.feature = None\n",
    "        self.gradient = None\n",
    "    \n",
    "    def save_gradient(self, grad):\n",
    "        self.gradient = grad\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        image_size = (x.size(-1), x.size(-2))\n",
    "        feature_maps =[]\n",
    "        \n",
    "        for i in range(x.size(0)):\n",
    "            img = x[i].data.cpu().numpy() #GPU上のTensorはcpuに移さないとnumpyに変換できない\n",
    "            img = img - np.min(img)\n",
    "            if np.max(img) != 0:\n",
    "                img = img / np.max(img)\n",
    "            \n",
    "            feature = x[i].unsqueeze(0)\n",
    "            \n",
    "            for name, module in self.model.named_children():\n",
    "                if name == 'clasifier':\n",
    "                    feature = feature.view(feature.size(0), -1)\n",
    "                feature = module(feature)\n",
    "                if name == 'features':\n",
    "                    feature.register_hook(self.save_gradient)\n",
    "                    self.feature = feature\n",
    "                    \n",
    "            classes = F.sigmoid(feature)\n",
    "            one_hot, _ = classes.max(dim=-1)\n",
    "            self.model.zero_grad()\n",
    "            one_hot.backward()\n",
    "            \n",
    "            weight = self.gradient.mean(dim=-1, keppdim-True).mean(dim=-2, keepdim=True)\n",
    "            \n",
    "            mask = F.relu((weight*self.feature).sum(dim=1)).squeeze(0)\n",
    "            mask = cv2.resize(mask.data.cpu().numpy(), image_size)\n",
    "            mask = mask - np.min(mask)\n",
    "            \n",
    "            if np.max(mask) != 0:\n",
    "                mask = mask/np.max(mask)\n",
    "                \n",
    "            feature_map = np.float32(cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET))\n",
    "            cam = feature_map + np.float32((np.uint8(img.transpose((1,2,0))*225)))\n",
    "            cam = cam - np.min(cam)\n",
    "            \n",
    "            if np.max(cam) != 0:\n",
    "                cam = cam/np.max(cam)\n",
    "                \n",
    "            feature_maps.append(transforms.ToTensor()(cv2.cvtColor(np.uint8(225*cam), cv2.COLOR_BGR2RGB)))\n",
    "            \n",
    "        feature_maps = torch.stack(feature_maps)\n",
    "        \n",
    "        return feature_maps\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cam_test_img_path)):\n",
    "    #入力画像の読み込み\n",
    "    cam_test_img = Image.open(cam_test_img_path[i])\n",
    "    cam_img_tensor = (data_transform['data']((cam_test_img))).unsqueeze(dim=0)\n",
    "    \n",
    "    img_size = cam_test_img.size\n",
    "    #grad-camによる予測根拠可視化\n",
    "    gradcam = GradCam(model\n",
    "    \n",
    "    feature_image = gradcam(cam_img_tensor).squeeze(dim=0)\n",
    "    feature_image = transforms.ToPILImage()(feature_image)\n",
    "    \n",
    "    pred_idx = model(cam_img_tensor).max(1)[1]\n",
    "                      \n",
    "    save_dir = '../data/gradcam_img/VGG16/'+s_classlist[i]\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\t# Make a directory\n",
    "    #保存先ディレクトリ名はその画像のクラス，画像の予測値を画像の名前に書き込む\n",
    "    cv2.imwrite(save_dir+'/heatmap_pred_'+s_classlist[pred_idx]+'.jpg', superimposed_img)\n",
    "    print('Saved: ', save_dir+'/heatmap_pred_'+s_classlist[pred_idx]+'.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
